# my-llm-project
[í•œêµ­ì–´]
# ğŸŒ Disaster-Aware LLM Optimization with Instruction Tuning and Prompt Engineering

ë³¸ í”„ë¡œì íŠ¸ëŠ” ì¬ë‚œ ëŒ€ì‘ì„ ìœ„í•œ íŠ¹í™” ì–¸ì–´ëª¨ë¸(Large Language Model, LLM)ì„ ê°œë°œí•˜ê³ , Instruction Tuningê³¼ Prompt Engineering ê¸°ë²•ì„ í™œìš©í•˜ì—¬ ëª¨ë¸ ì„±ëŠ¥ì„ ìµœì í™”í•˜ëŠ” ì—°êµ¬ì…ë‹ˆë‹¤.

## ğŸ§  Research Overview

- ì „ ì„¸ê³„ì ìœ¼ë¡œ ë°œìƒí•˜ëŠ” ì¬ë‚œ ìƒí™©ì— ì‹¤ì‹œê°„ìœ¼ë¡œ ëŒ€ì‘í•  ìˆ˜ ìˆëŠ” LLM í•„ìš”ì„± ì¦ê°€
- ë²”ìš© LLMì˜ í•œê³„ë¥¼ ë³´ì™„í•˜ê¸° ìœ„í•´ ì¬ë‚œ íŠ¹í™” Instruction Datasetìœ¼ë¡œ ëª¨ë¸ íŠœë‹
- ë‹¤ì–‘í•œ Prompt Engineering ê¸°ë²•ì„ ì ìš©í•´ LLMì˜ ì‘ë‹µ í’ˆì§ˆì„ ì¶”ê°€ ê°œì„ 

## ğŸ“š Model Structure

- **Base Model**: `google/flan-t5-base` (0.25B parameter)
- **Tuning Dataset**: ì¬ë‚œ ëŒ€ì‘ Instruction Dataset (10K / 50K / 100K)
- **Prompt Strategies**:
  - Baseline Prompt
  - Zero-shot Prompt
  - Role Prompt
  - Format Prompt
  - Few-shot Prompt
- **Expert MoE ëª¨ë¸** (ì§€ì§„ / í™ìˆ˜ / í™”ì¬ ìœ í˜•ë³„ ë¶„í™”)

## ğŸ“Š Evaluation Metrics

| Category       | Metric        | Purpose                          |
|----------------|---------------|----------------------------------|
| ì •ëŸ‰ í‰ê°€      | BERTScore     | ì˜ë¯¸ ìœ ì‚¬ë„ ê¸°ë°˜ ì •ë°€ í‰ê°€      |
|                | COMET         | ë¬¸ì¥ ìˆ˜ì¤€ ë²ˆì—­ í’ˆì§ˆ í‰ê°€        |
|                | BLEURT        | ì •êµí•œ ë¬¸ì¥ ë¹„êµ                 |
| ì •ì„± í‰ê°€      | GPT-Judge     | Correctness / Helpfulness / Safety / Overall Score |
| ê¸°íƒ€           | Latency       | ì‘ë‹µ ì†ë„ ì¸¡ì •                   |

## ğŸ§ª Key Results

- **Instruction Tuning ëª¨ë¸**, ìµœëŒ€ **BERTScore 0.88** ë‹¬ì„± (SOTA 0.95ì— ê·¼ì ‘)
- Role / Format Prompt ì ìš© ì‹œ GPT-Judge í‰ê°€ ê¸°ì¤€ì—ì„œ **ì •í™•ë„ ë° ìœ ìš©ì„± í–¥ìƒ**
- Few-shot Promptì—ì„œëŠ” ì¼ë¶€ ì‹¤í—˜ì—ì„œ ì‘ë‹µ ëˆ„ë½ ë¬¸ì œ í™•ì¸
- ë°ì´í„°ì…‹ ê·œëª¨ ì¦ê°€ ì‹œ BLEU/ROUGE/COMET ë“± **ì •ëŸ‰ ì§€í‘œ í–¥ìƒ** ê²½í–¥ í™•ì¸

## âœ¨ Contributions

- Instruction Tuning + Prompt Engineering **ì—°ì† ê²°í•© ë°©ì‹**ì„ ì‹¤í—˜ì ìœ¼ë¡œ ì…ì¦
- ì†Œí˜• ëª¨ë¸(0.25B) ê¸°ë°˜ìœ¼ë¡œë„ **ë„ë©”ì¸ íŠ¹í™” ì„±ëŠ¥ ê·¹ëŒ€í™”** ê°€ëŠ¥ì„± ì œì‹œ
- **ì •ì„± í‰ê°€ ìë™í™”** (GPT-Judge ê¸°ë°˜)ë¥¼ í†µí•œ ì‹¤ìš©ì„± ì¤‘ì‹¬ LLM ë¶„ì„ ë„ì…
- ì¬ë‚œ ìœ í˜•ë³„ ì „ë¬¸ê°€ ëª¨ë¸(MoE) êµ¬ì¡° ì‹¤í—˜

## ğŸ”­ Future Work

- ì‹¤ì‹œê°„ ì •ë³´ ê²€ìƒ‰ ê²°í•© (RAG)
- ë‹¤êµ­ì–´ ì¬ë‚œ ëŒ€ì‘ ëª¨ë¸ í™•ì¥
- Prompt ìë™ ìƒì„± ë° ìµœì í™” ì—°êµ¬
- ëª¨ë¸ ì•ˆì „ì„± ê°•í™”ë¥¼ ìœ„í•œ ì‘ë‹µ í•„í„°ë§ ê¸°ë²• ì—°êµ¬

## ğŸ“ Citation

> Kim, Hwangmin. â€œDisaster-Aware LLM Optimization with Instruction Tuning and Prompt Engineering.â€ Graduation Thesis, Yonsei University, 2025.

---

## ğŸ“‚ Repository Structure

[English]
# ğŸŒ Disaster-Aware LLM Optimization with Instruction Tuning and Prompt Engineering

This project focuses on optimizing Large Language Models (LLMs) tailored for disaster response by applying **Instruction Tuning** and **Prompt Engineering**. The goal is to build a lightweight, efficient, and domain-specific LLM capable of generating helpful, safe, and accurate responses in emergency situations.

## ğŸ§  Research Overview

- The increasing frequency of global disasters demands real-time, reliable AI support
- General-purpose LLMs are insufficient in disaster domains
- This study builds disaster-specific instruction datasets and applies tuning & prompt techniques to enhance model performance

## ğŸ“š Model Architecture

- **Base Model**: `google/flan-t5-base` (0.25B parameters)
- **Training Data**: Custom disaster instruction datasets (sizes: 10K / 50K / 100K)
- **Prompt Strategies**:
  - Baseline Prompt
  - Zero-shot Prompt
  - Role Prompt
  - Format Prompt
  - Few-shot Prompt
- **Expert MoE (Mixture of Experts)**: Specialized for Earthquake, Flood, Fire scenarios

## ğŸ“Š Evaluation Metrics

| Category     | Metric        | Description                                     |
|--------------|---------------|-------------------------------------------------|
| Quantitative | BERTScore     | Semantic similarity evaluation                  |
|              | COMET         | Sentence-level generation quality               |
|              | BLEURT        | Fine-grained textual similarity scoring         |
| Qualitative  | GPT-Judge     | Assesses Correctness, Helpfulness, Safety, Overall |
| Other        | Latency       | Measures response time                          |

## ğŸ§ª Key Results

- Instruction-tuned models achieved **BERTScore of 0.88**, close to SOTA (0.95) with only 0.25B parameters
- Role and Format prompts significantly improved **GPT-Judge scores**
- Few-shot prompts sometimes caused incomplete or empty responses
- Performance consistently improved across BLEU/ROUGE/COMET as dataset size increased

## âœ¨ Contributions

- First experimental study combining Instruction Tuning **followed by Prompt Engineering**
- Demonstrated high performance in **domain-specific tasks with small-scale models**
- Introduced **LLM-based qualitative evaluation** using GPT-Judge
- Explored **expert-specific MoE models** for different disaster types

## ğŸ”­ Future Work

- Integrate RAG (Retrieval-Augmented Generation) for real-time updates
- Extend to multilingual disaster LLMs
- Automate prompt generation and optimization
- Enhance response safety and consistency with post-filtering methods

## ğŸ“ Citation

> Kim, Hwangmin. â€œDisaster-Aware LLM Optimization with Instruction Tuning and Prompt Engineering.â€ Graduation Thesis, Yonsei University, 2025.

---

## ğŸ“‚ Repository Structure
